{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets, networks, sampling, completion, evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "num_epochs = 25\n",
    "\n",
    "# TODO Select a optimizer. [ 'adam', 'adamw', 'rmsprop' ]\n",
    "optimizer_option = 'adam'\n",
    "\n",
    "# TODO Select a lr scheduler. [ 'step', 'cosine', 'exponential']\n",
    "lr_scheduler_option = 'step'\n",
    "\n",
    "# TODO Select a batch size.\n",
    "batch_size = 64\n",
    "\n",
    "# TODO Select a learning rate.\n",
    "lr = 0.00001\n",
    "\n",
    "\n",
    "num_residual = 8\n",
    "num_kernels = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.Dataset('mnist', batch_size=batch_size)\n",
    "training_data = ds.get_train_data_loader()\n",
    "test_data = ds.get_test_data_loader()\n",
    "\n",
    "# ds.visualize_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "=========================================\n",
      "Epoch:   1/ 25, Batch     1, Loss: 5.5609\n",
      "Epoch:   1/ 25, Batch   101, Loss: 5.5005\n",
      "Epoch:   1/ 25, Batch   201, Loss: 5.3078\n",
      "Epoch:   1/ 25, Batch   301, Loss: 4.5070\n",
      "Epoch:   1/ 25, Batch   401, Loss: 2.3677\n",
      "Epoch:   1/ 25, Batch   501, Loss: 2.0467\n",
      "Epoch:   1/ 25, Batch   601, Loss: 1.8921\n",
      "Epoch:   1/ 25, Batch   701, Loss: 1.7636\n",
      "Epoch:   1/ 25, Batch   801, Loss: 1.6417\n",
      "Epoch:   1/ 25, Batch   901, Loss: 1.5363\n",
      "-----------------------------------------\n",
      "Epoch:   1 took 73.45s\n",
      "Epoch:   2/ 25, Batch     1, Loss: 1.4383\n",
      "Epoch:   2/ 25, Batch   101, Loss: 1.4073\n",
      "Epoch:   2/ 25, Batch   201, Loss: 1.3352\n",
      "Epoch:   2/ 25, Batch   301, Loss: 1.2782\n",
      "Epoch:   2/ 25, Batch   401, Loss: 1.2409\n",
      "Epoch:   2/ 25, Batch   501, Loss: 1.2134\n",
      "Epoch:   2/ 25, Batch   601, Loss: 1.1955\n",
      "Epoch:   2/ 25, Batch   701, Loss: 1.1790\n",
      "Epoch:   2/ 25, Batch   801, Loss: 1.1737\n",
      "Epoch:   2/ 25, Batch   901, Loss: 1.1605\n",
      "-----------------------------------------\n",
      "Epoch:   2 took 75.02s\n",
      "Epoch:   3/ 25, Batch     1, Loss: 1.1470\n",
      "Epoch:   3/ 25, Batch   101, Loss: 1.1542\n",
      "Epoch:   3/ 25, Batch   201, Loss: 1.1391\n",
      "Epoch:   3/ 25, Batch   301, Loss: 1.1324\n",
      "Epoch:   3/ 25, Batch   401, Loss: 1.1277\n",
      "Epoch:   3/ 25, Batch   501, Loss: 1.1201\n",
      "Epoch:   3/ 25, Batch   601, Loss: 1.1112\n",
      "Epoch:   3/ 25, Batch   701, Loss: 1.1116\n",
      "Epoch:   3/ 25, Batch   801, Loss: 1.0952\n",
      "Epoch:   3/ 25, Batch   901, Loss: 1.0948\n",
      "-----------------------------------------\n",
      "Epoch:   3 took 75.18s\n",
      "Epoch:   4/ 25, Batch     1, Loss: 1.1605\n",
      "Epoch:   4/ 25, Batch   101, Loss: 1.0883\n",
      "Epoch:   4/ 25, Batch   201, Loss: 1.0728\n",
      "Epoch:   4/ 25, Batch   301, Loss: 1.0738\n",
      "Epoch:   4/ 25, Batch   401, Loss: 1.0627\n",
      "Epoch:   4/ 25, Batch   501, Loss: 1.0670\n",
      "Epoch:   4/ 25, Batch   601, Loss: 1.0586\n",
      "Epoch:   4/ 25, Batch   701, Loss: 1.0583\n",
      "Epoch:   4/ 25, Batch   801, Loss: 1.0536\n",
      "Epoch:   4/ 25, Batch   901, Loss: 1.0543\n",
      "-----------------------------------------\n",
      "Epoch:   4 took 78.22s\n",
      "Epoch:   5/ 25, Batch     1, Loss: 1.0635\n",
      "Epoch:   5/ 25, Batch   101, Loss: 1.0521\n",
      "Epoch:   5/ 25, Batch   201, Loss: 1.0498\n",
      "Epoch:   5/ 25, Batch   301, Loss: 1.0491\n",
      "Epoch:   5/ 25, Batch   401, Loss: 1.0460\n",
      "Epoch:   5/ 25, Batch   501, Loss: 1.0462\n",
      "Epoch:   5/ 25, Batch   601, Loss: 1.0485\n",
      "Epoch:   5/ 25, Batch   701, Loss: 1.0434\n",
      "Epoch:   5/ 25, Batch   801, Loss: 1.0428\n",
      "Epoch:   5/ 25, Batch   901, Loss: 1.0497\n",
      "-----------------------------------------\n",
      "Epoch:   5 took 70.96s\n",
      "Epoch:   6/ 25, Batch     1, Loss: 1.0883\n",
      "Epoch:   6/ 25, Batch   101, Loss: 1.0389\n",
      "Epoch:   6/ 25, Batch   201, Loss: 1.0446\n",
      "Epoch:   6/ 25, Batch   301, Loss: 1.0437\n",
      "Epoch:   6/ 25, Batch   401, Loss: 1.0397\n",
      "Epoch:   6/ 25, Batch   501, Loss: 1.0416\n",
      "Epoch:   6/ 25, Batch   601, Loss: 1.0485\n",
      "Epoch:   6/ 25, Batch   701, Loss: 1.0372\n",
      "Epoch:   6/ 25, Batch   801, Loss: 1.0327\n",
      "Epoch:   6/ 25, Batch   901, Loss: 1.0335\n",
      "-----------------------------------------\n",
      "Epoch:   6 took 70.45s\n",
      "Epoch:   7/ 25, Batch     1, Loss: 1.0670\n",
      "Epoch:   7/ 25, Batch   101, Loss: 1.0449\n",
      "Epoch:   7/ 25, Batch   201, Loss: 1.0411\n",
      "Epoch:   7/ 25, Batch   301, Loss: 1.0315\n",
      "Epoch:   7/ 25, Batch   401, Loss: 1.0299\n",
      "Epoch:   7/ 25, Batch   501, Loss: 1.0353\n",
      "Epoch:   7/ 25, Batch   601, Loss: 1.0328\n",
      "Epoch:   7/ 25, Batch   701, Loss: 1.0347\n",
      "Epoch:   7/ 25, Batch   801, Loss: 1.0371\n",
      "Epoch:   7/ 25, Batch   901, Loss: 1.0434\n",
      "-----------------------------------------\n",
      "Epoch:   7 took 70.63s\n",
      "Epoch:   8/ 25, Batch     1, Loss: 1.0054\n",
      "Epoch:   8/ 25, Batch   101, Loss: 1.0337\n",
      "Epoch:   8/ 25, Batch   201, Loss: 1.0367\n",
      "Epoch:   8/ 25, Batch   301, Loss: 1.0352\n",
      "Epoch:   8/ 25, Batch   401, Loss: 1.0372\n",
      "Epoch:   8/ 25, Batch   501, Loss: 1.0345\n",
      "Epoch:   8/ 25, Batch   601, Loss: 1.0382\n",
      "Epoch:   8/ 25, Batch   701, Loss: 1.0369\n",
      "Epoch:   8/ 25, Batch   801, Loss: 1.0301\n",
      "Epoch:   8/ 25, Batch   901, Loss: 1.0308\n",
      "-----------------------------------------\n",
      "Epoch:   8 took 69.98s\n",
      "Epoch:   9/ 25, Batch     1, Loss: 1.0339\n",
      "Epoch:   9/ 25, Batch   101, Loss: 1.0320\n",
      "Epoch:   9/ 25, Batch   201, Loss: 1.0348\n",
      "Epoch:   9/ 25, Batch   301, Loss: 1.0307\n",
      "Epoch:   9/ 25, Batch   401, Loss: 1.0355\n",
      "Epoch:   9/ 25, Batch   501, Loss: 1.0331\n",
      "Epoch:   9/ 25, Batch   601, Loss: 1.0357\n",
      "Epoch:   9/ 25, Batch   701, Loss: 1.0280\n",
      "Epoch:   9/ 25, Batch   801, Loss: 1.0294\n",
      "Epoch:   9/ 25, Batch   901, Loss: 1.0282\n",
      "-----------------------------------------\n",
      "Epoch:   9 took 71.12s\n",
      "Epoch:  10/ 25, Batch     1, Loss: 1.0974\n",
      "Epoch:  10/ 25, Batch   101, Loss: 1.0374\n",
      "Epoch:  10/ 25, Batch   201, Loss: 1.0331\n",
      "Epoch:  10/ 25, Batch   301, Loss: 1.0292\n",
      "Epoch:  10/ 25, Batch   401, Loss: 1.0314\n",
      "Epoch:  10/ 25, Batch   501, Loss: 1.0263\n",
      "Epoch:  10/ 25, Batch   601, Loss: 1.0294\n",
      "Epoch:  10/ 25, Batch   701, Loss: 1.0311\n",
      "Epoch:  10/ 25, Batch   801, Loss: 1.0286\n",
      "Epoch:  10/ 25, Batch   901, Loss: 1.0286\n",
      "-----------------------------------------\n",
      "Epoch:  10 took 74.22s\n",
      "Epoch:  11/ 25, Batch     1, Loss: 0.9726\n",
      "Epoch:  11/ 25, Batch   101, Loss: 1.0301\n",
      "Epoch:  11/ 25, Batch   201, Loss: 1.0265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m training_data:\n\u001b[0;32m     66\u001b[0m   target \u001b[38;5;241m=\u001b[39m Variable(images[:,\u001b[38;5;241m0\u001b[39m,:,:]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 67\u001b[0m   images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m   target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     71\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR\n",
    "from torch.optim import Adam, AdamW, RMSprop, SGD \n",
    "from torch.autograd import Variable\n",
    "\n",
    "PixelCNN = networks.PixelCNN(num_kernels=num_kernels)\n",
    "# Select device\n",
    "# if torch.cuda.is_available():\n",
    "#   device = torch.device('cuda:0')\n",
    "# else:\n",
    "#   device = torch.device('cpu')\n",
    "device = torch.device('cuda:0')\n",
    "print(f'Using device: {device}')\n",
    "print('=========================================')\n",
    "\n",
    "PixelCNN.to(device)\n",
    "\n",
    "learning_rates = []\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_loss_epochs = []\n",
    "test_loss_epochs = []\n",
    "optimizer = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select optimizer\n",
    "if optimizer_option == \"adam\":\n",
    "  optimizer = torch.optim.Adam(PixelCNN.parameters(), lr)\n",
    "  \n",
    "elif optimizer_option == \"adamw\":\n",
    "  optimizer = torch.optim.AdamW(PixelCNN.parameters(), lr)\n",
    "  \n",
    "elif optimizer_option == \"rmsprop\":\n",
    "  optimizer = torch.optim.RMSprop(PixelCNN.parameters(), lr)\n",
    "  \n",
    "else:\n",
    "  optimizer = torch.optim.SGD(PixelCNN.parameters(), lr, momentum=0.9)  \n",
    "\n",
    "\n",
    "#Select scheduler\n",
    "scheduler = None\n",
    "if lr_scheduler_option == 'step':\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "    \n",
    "elif lr_scheduler_option == 'exponential':\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "elif lr_scheduler_option == 'cosine':\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  epoch_start_time = time.time()\n",
    "  # Switch to training mode.\n",
    "  PixelCNN.train()\n",
    "\n",
    "  losses = []\n",
    "  batch_idx = 0\n",
    "\n",
    "  for images, labels in training_data:\n",
    "\n",
    "    target = Variable(images[:,0,:,:]*255).long()\n",
    "    # target = images.view(-1)\n",
    "    images = images.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = PixelCNN(images)\n",
    "\n",
    "    # output.reshape(-1, 256)\n",
    "\n",
    "    # print(output.shape)\n",
    "    # print(target.shape)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(PixelCNN.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "      learning_rates.append(param_group['lr'])\n",
    "\n",
    "\n",
    "\n",
    "    losses.append(loss.detach().clone())\n",
    "\n",
    "    if batch_idx % 100 == 0:\n",
    "      average_loss = torch.stack(losses).mean().item()\n",
    "      train_loss_curve.append(average_loss)\n",
    "      train_loss_epochs.append(epoch + 1)\n",
    "      losses = []\n",
    "      print(f'Epoch: {epoch + 1:3d}/{num_epochs:3d}, Batch {batch_idx + 1:5d}, Loss: {average_loss:.4f}')\n",
    "    batch_idx += 1\n",
    "\n",
    "  # scheduler.step()\n",
    "  epoch_end_time = time.time()\n",
    "  print('-----------------------------------------')\n",
    "  print(f'Epoch: {epoch + 1:3d} took {epoch_end_time - epoch_start_time:.2f}s')\n",
    "  # test_loss = evaluation.evaluate(model=PixelCNN, test_data_loader=test_data, device=device, batch_size=batch_size)\n",
    "  # test_loss_curve.append(test_loss)\n",
    "  # test_loss_epochs.append(epoch + 1)\n",
    "  # print(f'Epoch: {epoch + 1:3d}, Test Loss: {test_loss:.4f}')\n",
    "  # print('-----------------------------------------')\n",
    "  \n",
    "\n",
    "  sampling.samplemnist(PixelCNN, num_samples=5)\n",
    "\n",
    "  \n",
    "\n",
    "overall_end_time = time.time()\n",
    "print('=========================================')\n",
    "print(f'Training took {overall_end_time - overall_start_time:.2f}s')\n",
    "\n",
    "# Loss Curve Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_epochs, train_loss_curve, label='Train Loss')\n",
    "plt.scatter(test_loss_epochs, test_loss_curve, color='red', label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Learning Rate Plot\n",
    "num_batches = len(training_data)\n",
    "learning_rates_res = [sum(learning_rates[i * num_batches:(i + 1) * num_batches]) / num_batches for i in range(num_epochs)]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), learning_rates_res)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate over Time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
