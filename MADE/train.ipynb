{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision\n","import datasets, networks, sampling\n","# We use matplotlib to plot the loss curve.\n","import matplotlib.pyplot as plt\n","# Use shorthands (nn instead of torch.nn, optim instead of torch.optim)\n","from torch import nn, optim"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","=========================================\n"]},{"ename":"TypeError","evalue":"linear(): argument 'input' (position 1) must be Tensor, not DataLoader","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 96\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# At this point, you need to implement the following steps:\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# 1. Compute the outputs of our neural network (don't use predict()!).\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# 2. Compute the cross-entropy loss from nn.functional.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m##############################\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# TODO Write your code here.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m optimizer\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(MADE\u001b[38;5;241m.\u001b[39mparameters(),lr)\n\u001b[1;32m---> 96\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mMADE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(output,labels)\n\u001b[0;32m     98\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\NiciG\\OneDrive\\Desktop\\SoPra\\G1\\MADE\\networks.py:32\u001b[0m, in \u001b[0;36mMADE.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\NiciG\\anaconda3\\envs\\Fachprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\NiciG\\anaconda3\\envs\\Fachprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\NiciG\\anaconda3\\envs\\Fachprojekt\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[1;32mc:\\Users\\NiciG\\anaconda3\\envs\\Fachprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\NiciG\\anaconda3\\envs\\Fachprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\NiciG\\OneDrive\\Desktop\\SoPra\\G1\\MADE\\networks.py:54\u001b[0m, in \u001b[0;36mMaskedLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not DataLoader"]}],"source":["# The following settings are called \"hyperparameters\", which need to be selected\n","# by you (a human), usually through trial and error.\n","\n","# One epoch means seeing every image of the training dataset, which consists of 60,000 images.\n","# If the number is too low, the network will not be able to learn well.\n","# Seeing 5 times each image should be enough to reach >96% accuracy. (We get ~98%).\n","# Don`t change this value, we will use 5 epochs.\n","num_epochs = 5 # don't change this value\n","\n","# The batch size is the number of images per update of the network.\n","##############################\n","##############################\n","# TODO Select a batch size.\n","batch_size = 32\n","##############################\n","##############################\n","\n","# The learning rate determines how drastically the parameters of the network\n","# change. This value needs to be selected carefully. \n","##############################\n","##############################\n","# TODO Select a learning rate.\n","lr= 0.0001\n","##############################\n","##############################\n","\n","num_units=500\n","num_layer=5\n","\n","\n","# Create \"data loaders\" for training and testing with the batch size from above.\n","# They can do things like multiprocessing and shuffling the order of the images.\n","# We can iterate over them to obtain batches of images and labels\n","# (see training loop below).\n","##############################\n","# TODO Write your code here.\n","Ds = datasets.Dataset(batch_size=batch_size)\n","training_data = Ds.get_train_data_loader()\n","test_data = Ds.get_test_data_loader()\n","\n","\n","ordering=range(1,785)\n","\n","##############################\n","\n","# Create the neural network and its optimizer.\n","##############################\n","##############################\n","# TODO Write your code here.\n","MADE= networks.MADE(num_layer=num_layer, num_units=num_units, ordering=ordering, input_feat=28*28)\n","##############################\n","##############################\n","\n","# Select the device that will be used for training.\n","# This code selects a GPU, if it is available, otherwise the CPU.\n","if torch.cuda.is_available():\n","  device = torch.device('cuda:0')\n","else:\n","  device = torch.device('cpu')\n","print(f'Using device: {device}')\n","print('=========================================')\n","\n","MADE.to(device)  # Put the neural network on the selected device. GPU or CPU.\n","\n","# We want to plot the loss and accuracy curve at the end of training.\n","loss_curve = []\n","accuracy_curve = []\n","epoch_markers = []\n","\n","for epoch in range(num_epochs):\n","  # Start a new epoch and iterate over all images in the training dataset.\n","\n","  # Switch to training mode.\n","  MADE.train()\n","\n","  # We want to print the average loss now and then during the epoch.\n","  losses = []\n","  batch_idx = 0\n","\n","  for images, labels in training_data:\n","    # images and labels are tensors.\n","    # Move the tensors to the device.\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # At this point, you need to implement the following steps:\n","    # 1. Compute the outputs of our neural network (don't use predict()!).\n","    # 2. Compute the cross-entropy loss from nn.functional.\n","    # 3. Compute the gradients of the loss function with respect to all parameters.\n","    # 4. Update all parameters using the optimizer.\n","    # 5. Reset the gradients, we don't want them anymore (important!).\n","    ##############################\n","    ##############################\n","    # TODO Write your code here.\n","    optimizer= torch.optim.Adam(MADE.parameters(),lr)\n","    output=MADE.forward(images)\n","    loss = nn.functional.cross_entropy(output,labels)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    ##############################\n","    ##############################\n","\n","    # Save the loss value.\n","    losses.append(loss.detach().clone())\n","    # Print the average loss now and then.\n","    if batch_idx % 100 == 0:\n","      average_loss = torch.stack(losses).mean().item()\n","      loss_curve.append(average_loss)\n","      losses = []\n","      print(f'Epoch: {epoch + 1:3d}/{num_epochs:3d}, Batch {batch_idx + 1:5d}, Loss: {average_loss:.4f}')\n","    batch_idx += 1\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Fachprojekt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":2}
