{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision\n","import datasets, networks, sampling\n","# We use matplotlib to plot the loss curve.\n","import matplotlib.pyplot as plt\n","# Use shorthands (nn instead of torch.nn, optim instead of torch.optim)\n","from torch import nn, optim"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda:0\n","=========================================\n","Epoch:   1/  5, Batch     1, Loss: 0.7093\n","Epoch:   1/  5, Batch   101, Loss: 0.4566\n","Epoch:   1/  5, Batch   201, Loss: 0.2625\n","Epoch:   1/  5, Batch   301, Loss: 0.2109\n","Epoch:   1/  5, Batch   401, Loss: 0.1864\n","Epoch:   1/  5, Batch   501, Loss: 0.1689\n","Epoch:   1/  5, Batch   601, Loss: 0.1577\n","Epoch:   1/  5, Batch   701, Loss: 0.1484\n","Epoch:   1/  5, Batch   801, Loss: 0.1394\n","Epoch:   1/  5, Batch   901, Loss: 0.1338\n","Epoch:   1/  5, Batch  1001, Loss: 0.1264\n","Epoch:   1/  5, Batch  1101, Loss: 0.1220\n","Epoch:   1/  5, Batch  1201, Loss: 0.1182\n","Epoch:   1/  5, Batch  1301, Loss: 0.1133\n","Epoch:   1/  5, Batch  1401, Loss: 0.1103\n","Epoch:   1/  5, Batch  1501, Loss: 0.1071\n","Epoch:   1/  5, Batch  1601, Loss: 0.1036\n","Epoch:   1/  5, Batch  1701, Loss: 0.0995\n","Epoch:   1/  5, Batch  1801, Loss: 0.0970\n","Epoch:   2/  5, Batch     1, Loss: 0.0927\n","Epoch:   2/  5, Batch   101, Loss: 0.0918\n","Epoch:   2/  5, Batch   201, Loss: 0.0900\n","Epoch:   2/  5, Batch   301, Loss: 0.0879\n","Epoch:   2/  5, Batch   401, Loss: 0.0866\n","Epoch:   2/  5, Batch   501, Loss: 0.0845\n","Epoch:   2/  5, Batch   601, Loss: 0.0826\n","Epoch:   2/  5, Batch   701, Loss: 0.0805\n","Epoch:   2/  5, Batch   801, Loss: 0.0794\n","Epoch:   2/  5, Batch   901, Loss: 0.0779\n","Epoch:   2/  5, Batch  1001, Loss: 0.0755\n","Epoch:   2/  5, Batch  1101, Loss: 0.0741\n","Epoch:   2/  5, Batch  1201, Loss: 0.0735\n","Epoch:   2/  5, Batch  1301, Loss: 0.0726\n","Epoch:   2/  5, Batch  1401, Loss: 0.0707\n","Epoch:   2/  5, Batch  1501, Loss: 0.0694\n","Epoch:   2/  5, Batch  1601, Loss: 0.0686\n","Epoch:   2/  5, Batch  1701, Loss: 0.0671\n","Epoch:   2/  5, Batch  1801, Loss: 0.0657\n","Epoch:   3/  5, Batch     1, Loss: 0.0660\n","Epoch:   3/  5, Batch   101, Loss: 0.0634\n","Epoch:   3/  5, Batch   201, Loss: 0.0629\n","Epoch:   3/  5, Batch   301, Loss: 0.0622\n","Epoch:   3/  5, Batch   401, Loss: 0.0618\n","Epoch:   3/  5, Batch   501, Loss: 0.0608\n","Epoch:   3/  5, Batch   601, Loss: 0.0599\n","Epoch:   3/  5, Batch   701, Loss: 0.0586\n","Epoch:   3/  5, Batch   801, Loss: 0.0581\n","Epoch:   3/  5, Batch   901, Loss: 0.0583\n","Epoch:   3/  5, Batch  1001, Loss: 0.0569\n","Epoch:   3/  5, Batch  1101, Loss: 0.0573\n","Epoch:   3/  5, Batch  1201, Loss: 0.0555\n","Epoch:   3/  5, Batch  1301, Loss: 0.0551\n","Epoch:   3/  5, Batch  1401, Loss: 0.0539\n","Epoch:   3/  5, Batch  1501, Loss: 0.0541\n","Epoch:   3/  5, Batch  1601, Loss: 0.0541\n","Epoch:   3/  5, Batch  1701, Loss: 0.0528\n","Epoch:   3/  5, Batch  1801, Loss: 0.0525\n","Epoch:   4/  5, Batch     1, Loss: 0.0525\n","Epoch:   4/  5, Batch   101, Loss: 0.0503\n","Epoch:   4/  5, Batch   201, Loss: 0.0509\n","Epoch:   4/  5, Batch   301, Loss: 0.0499\n","Epoch:   4/  5, Batch   401, Loss: 0.0491\n","Epoch:   4/  5, Batch   501, Loss: 0.0493\n","Epoch:   4/  5, Batch   601, Loss: 0.0485\n","Epoch:   4/  5, Batch   701, Loss: 0.0475\n","Epoch:   4/  5, Batch   801, Loss: 0.0476\n","Epoch:   4/  5, Batch   901, Loss: 0.0474\n","Epoch:   4/  5, Batch  1001, Loss: 0.0473\n","Epoch:   4/  5, Batch  1101, Loss: 0.0463\n","Epoch:   4/  5, Batch  1201, Loss: 0.0457\n","Epoch:   4/  5, Batch  1301, Loss: 0.0461\n","Epoch:   4/  5, Batch  1401, Loss: 0.0458\n","Epoch:   4/  5, Batch  1501, Loss: 0.0452\n","Epoch:   4/  5, Batch  1601, Loss: 0.0442\n","Epoch:   4/  5, Batch  1701, Loss: 0.0453\n","Epoch:   4/  5, Batch  1801, Loss: 0.0441\n","Epoch:   5/  5, Batch     1, Loss: 0.0364\n","Epoch:   5/  5, Batch   101, Loss: 0.0428\n","Epoch:   5/  5, Batch   201, Loss: 0.0428\n","Epoch:   5/  5, Batch   301, Loss: 0.0425\n","Epoch:   5/  5, Batch   401, Loss: 0.0416\n","Epoch:   5/  5, Batch   501, Loss: 0.0421\n","Epoch:   5/  5, Batch   601, Loss: 0.0418\n","Epoch:   5/  5, Batch   701, Loss: 0.0409\n","Epoch:   5/  5, Batch   801, Loss: 0.0411\n","Epoch:   5/  5, Batch   901, Loss: 0.0407\n","Epoch:   5/  5, Batch  1001, Loss: 0.0401\n","Epoch:   5/  5, Batch  1101, Loss: 0.0398\n","Epoch:   5/  5, Batch  1201, Loss: 0.0403\n","Epoch:   5/  5, Batch  1301, Loss: 0.0401\n","Epoch:   5/  5, Batch  1401, Loss: 0.0398\n","Epoch:   5/  5, Batch  1501, Loss: 0.0390\n","Epoch:   5/  5, Batch  1601, Loss: 0.0395\n","Epoch:   5/  5, Batch  1701, Loss: 0.0391\n","Epoch:   5/  5, Batch  1801, Loss: 0.0387\n"]}],"source":["# The following settings are called \"hyperparameters\", which need to be selected\n","# by you (a human), usually through trial and error.\n","\n","# One epoch means seeing every image of the training dataset, which consists of 60,000 images.\n","# If the number is too low, the network will not be able to learn well.\n","# Seeing 5 times each image should be enough to reach >96% accuracy. (We get ~98%).\n","# Don`t change this value, we will use 5 epochs.\n","num_epochs = 5 # don't change this value\n","\n","# The batch size is the number of images per update of the network.\n","##############################\n","##############################\n","# TODO Select a batch size.\n","batch_size = 32\n","##############################\n","##############################\n","\n","# The learning rate determines how drastically the parameters of the network\n","# change. This value needs to be selected carefully. \n","##############################\n","##############################\n","# TODO Select a learning rate.\n","lr= 0.0001\n","##############################\n","##############################\n","\n","num_units=500\n","num_layer=5\n","\n","\n","# Create \"data loaders\" for training and testing with the batch size from above.\n","# They can do things like multiprocessing and shuffling the order of the images.\n","# We can iterate over them to obtain batches of images and labels\n","# (see training loop below).\n","##############################\n","# TODO Write your code here.\n","Ds = datasets.Dataset('mnist', batch_size=batch_size)\n","training_data = Ds.get_train_data_loader()\n","test_data = Ds.get_test_data_loader()\n","\n","\n","ordering=range(1,785)\n","\n","##############################\n","\n","# Create the neural network and its optimizer.\n","##############################\n","##############################\n","# TODO Write your code here.\n","MADE = networks.MADE(num_layer=num_layer, num_units=num_units, ordering=ordering, input_feat=28*28)\n","##############################\n","##############################\n","\n","# Select the device that will be used for training.\n","# This code selects a GPU, if it is available, otherwise the CPU.\n","if torch.cuda.is_available():\n","  device = torch.device('cuda:0')\n","else:\n","  device = torch.device('cpu')\n","print(f'Using device: {device}')\n","print('=========================================')\n","\n","MADE.to(device)  # Put the neural network on the selected device. GPU or CPU.\n","\n","# We want to plot the loss and accuracy curve at the end of training.\n","loss_curve = []\n","accuracy_curve = []\n","epoch_markers = []\n","optimizer= torch.optim.Adam(MADE.parameters(),lr)\n","\n","for epoch in range(num_epochs):\n","  # Start a new epoch and iterate over all images in the training dataset.\n","\n","  # Switch to training mode.\n","  MADE.train()\n","\n","  # We want to print the average loss now and then during the epoch.\n","  losses = []\n","  batch_idx = 0\n","\n","  for images, labels in training_data:\n","    # images and labels are tensors.\n","    # Move the tensors to the device.\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # At this point, you need to implement the following steps:\n","    # 1. Compute the outputs of our neural network (don't use predict()!).\n","    # 2. Compute the cross-entropy loss from nn.functional.\n","    # 3. Compute the gradients of the loss function with respect to all parameters.\n","    # 4. Update all parameters using the optimizer.\n","    # 5. Reset the gradients, we don't want them anymore (important!).\n","    ##############################\n","    ##############################\n","    # TODO Write your code here.\n","\n","    output=MADE(images)\n","    loss=nn.functional.binary_cross_entropy(output,images)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    ##############################\n","    ##############################\n","\n","    # Save the loss value.\n","    losses.append(loss.detach().clone())\n","    # Print the average loss now and then.\n","    if batch_idx % 100 == 0:\n","      average_loss = torch.stack(losses).mean().item()\n","      loss_curve.append(average_loss)\n","      losses = []\n","      print(f'Epoch: {epoch + 1:3d}/{num_epochs:3d}, Batch {batch_idx + 1:5d}, Loss: {average_loss:.4f}')\n","    batch_idx += 1\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Fachprojekt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":2}
